{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "讀取資料"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_15096\\1733714236.py:3: DtypeWarning: Columns (11,12,13,14,15,16,17,18,19,20,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  train_data = pd.read_csv(\"train_preprocessed.csv\")\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv(\"train_preprocessed.csv\")\n",
        "test_data = pd.read_csv(\"test_preprocessed.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "特徵工程 - 訓練集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = train_data.rename(columns={'locale_x':'locale', 'locale_name_x':'locale_name', 'description_x':'description', 'transferred_x':'transferred'})\n",
        "train_data.drop(['locale_y', 'locale_name_y', 'description_y', 'transferred_y'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['id', 'date', 'store_nbr', 'family', 'onpromotion', 'dcoilwtico',\n",
            "       'city', 'state', 'store_type', 'cluster', 'transactions', 'event_type',\n",
            "       'locale', 'locale_name', 'description', 'transferred', 'type', 'year',\n",
            "       'month', 'day', 'day_of_week', 'family_bert_embeddings',\n",
            "       'description_bert_embeddings', 'longitude', 'latitude', 'sales',\n",
            "       'isHoliday', 'isEvent'],\n",
            "      dtype='object')\n",
            "Index(['id', 'date', 'store_nbr', 'family', 'onpromotion', 'dcoilwtico',\n",
            "       'city', 'state', 'store_type', 'cluster', 'transactions', 'event_type',\n",
            "       'locale', 'locale_name', 'description', 'transferred', 'year', 'month',\n",
            "       'day', 'day_of_week', 'family_bert_embeddings',\n",
            "       'description_bert_embeddings', 'longitude', 'latitude', 'isHoliday',\n",
            "       'isEvent'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(train_data.columns)\n",
        "print(test_data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 建立處理嵌入向量欄位函數\n",
        "def process_embeddings(df, column_name):\n",
        "    embeddings = df[column_name].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=',')).values\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "# 批量處理嵌入向量，避免記憶體問題\n",
        "batch_size = 1000\n",
        "n_batches = len(train_data) // batch_size + 1\n",
        "\n",
        "family_embeddings = []\n",
        "description_embeddings = []\n",
        "\n",
        "for i in range(n_batches):\n",
        "    batch_data = train_data.iloc[i * batch_size: (i + 1) * batch_size]\n",
        "    family_embeddings.append(process_embeddings(batch_data, 'family_bert_embeddings'))\n",
        "    description_embeddings.append(process_embeddings(batch_data, 'description_bert_embeddings'))\n",
        "\n",
        "family_embeddings = np.vstack(family_embeddings)\n",
        "description_embeddings = np.vstack(description_embeddings)\n",
        "\n",
        "# 使用 PCA 將 Word2Vec 向量降維\n",
        "pca = PCA(n_components=10)\n",
        "family_reduced = pca.fit_transform(family_embeddings)\n",
        "description_reduced = pca.fit_transform(description_embeddings)\n",
        "\n",
        "# 將降維後的向量轉換為 DataFrame\n",
        "family_reduced_df = pd.DataFrame(family_reduced, columns=[f'family_{i}' for i in range(10)])\n",
        "description_reduced_df = pd.DataFrame(description_reduced, columns=[f'description_{i}' for i in range(10)])\n",
        "\n",
        "# 合併嵌入向量欄位到原始資料\n",
        "train_data = pd.concat([train_data, family_reduced_df, description_reduced_df], axis=1)\n",
        "train_data.drop(['family_bert_embeddings', 'description_bert_embeddings'], axis=1, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 選擇特徵和目標變數\n",
        "features = ['onpromotion', 'dcoilwtico', 'transactions', 'transferred', 'year', 'month', 'day', 'day_of_week', 'longitude', 'latitude', 'isHoliday', 'isEvent','store_nbr', 'store_type', 'cluster']\n",
        "features += [f'family_{i}' for i in range(10)] + [f'description_{i}' for i in range(10)]\n",
        "target = 'sales'\n",
        "\n",
        "# 處理缺失值\n",
        "train_data = train_data.fillna(0)\n",
        "\n",
        "# 選取需要的欄位資料\n",
        "feature_df = train_data[features]\n",
        "\n",
        "# 將資料集中的 'True' 和 'False' 取代為 1 和 0\n",
        "feature_df = feature_df.replace('True', 1)\n",
        "feature_df = feature_df.replace('False', 0)\n",
        "\n",
        "# one-hot encoding\n",
        "feature_df = pd.get_dummies(feature_df, columns=['store_nbr', 'store_type', 'cluster'])\n",
        "\n",
        "# 特徵和目標變數\n",
        "X = feature_df\n",
        "y = train_data[target]\n",
        "\n",
        "# 訓練集和測試集分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "模型評估"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Multi-index Linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Simple Linear Regression RMSLE: 2.702412208300194\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "\n",
        "# 初始化線性迴歸模型\n",
        "linear_model = LinearRegression()\n",
        "\n",
        "# 訓練模型\n",
        "linear_model.fit(X_train, y_train)\n",
        "\n",
        "# 後處理函數（預測結果最小為0）\n",
        "def postprocess_predictions(predictions):\n",
        "    return np.maximum(predictions, 0)\n",
        "\n",
        "# 驗證\n",
        "y_pred = linear_model.predict(X_val)\n",
        "y_pred = postprocess_predictions(y_pred)\n",
        "\n",
        "# 評估模型\n",
        "rmsle = np.sqrt(mean_squared_log_error(y_val, y_pred))\n",
        "print(f'Simple Linear Regression RMSLE: {rmsle}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         Feature    Importance\n",
            "66  store_type_A  6.253517e+13\n",
            "65  store_nbr_54  5.787804e+13\n",
            "76     cluster_6  5.766866e+13\n",
            "27  store_nbr_16  4.980347e+13\n",
            "16   store_nbr_5  4.535045e+13\n",
            "..           ...           ...\n",
            "61  store_nbr_50 -3.964648e+13\n",
            "60  store_nbr_49 -4.064981e+13\n",
            "56  store_nbr_45 -4.064981e+13\n",
            "55  store_nbr_44 -4.521478e+13\n",
            "68  store_type_C -4.819928e+13\n",
            "\n",
            "[88 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "# 觀察特徵模型特徵重要性\n",
        "\n",
        "# 獲取特徵重要性\n",
        "importance = linear_model.coef_\n",
        "\n",
        "# 將特徵名稱和其重要性存入DataFrame\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': importance\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(feature_importance_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Moving Average (MA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moving Average Model RMSLE: 3.4703451191046093\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_15096\\1081779447.py:6: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  moving_average_pred = moving_average_pred.fillna(method='bfill')\n"
          ]
        }
      ],
      "source": [
        "# 選定視窗大小初始化模型\n",
        "window_size = 3\n",
        "moving_average_pred = y_val.rolling(window=window_size).mean().shift(1)\n",
        "\n",
        "# 填補NaN值\n",
        "moving_average_pred = moving_average_pred.fillna(method='bfill')\n",
        "\n",
        "# 評估模型\n",
        "rmsle = np.sqrt(mean_squared_log_error(y_val, moving_average_pred))\n",
        "print(f'Moving Average Model RMSLE: {rmsle}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Exponential Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:917: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
            "  warnings.warn(\n",
            "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
            "  return get_prediction_index(\n",
            "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
            "  return get_prediction_index(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exponential Smoothing Model RMSLE: 3.8130367540262813\n"
          ]
        }
      ],
      "source": [
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "\n",
        "# 初始化模型\n",
        "exponential_smoothing_model = ExponentialSmoothing(y_train, seasonal='add', seasonal_periods=12).fit()\n",
        "y_exp_pred = exponential_smoothing_model.forecast(len(y_val))\n",
        "\n",
        "# 評估模型\n",
        "rmsle = np.sqrt(mean_squared_log_error(y_val, y_exp_pred))\n",
        "print(f'Exponential Smoothing Model RMSLE: {rmsle}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ARIMA Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2027: RuntimeWarning: overflow encountered in divide\n",
            "  s = divide(1, s, where=large, out=s)\n"
          ]
        },
        {
          "ename": "MemoryError",
          "evalue": "Unable to allocate 39.7 MiB for an array with shape (5, 1040682) and data type float64",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 初始化模型\u001b[39;00m\n\u001b[0;32m      4\u001b[0m arima_model \u001b[38;5;241m=\u001b[39m ARIMA(y_train, order\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m arima_model_fit \u001b[38;5;241m=\u001b[39m \u001b[43marima_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m y_arima_pred \u001b[38;5;241m=\u001b[39m arima_model_fit\u001b[38;5;241m.\u001b[39mforecast(steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(y_val))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 評估模型\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\arima\\model.py:395\u001b[0m, in \u001b[0;36mARIMA.fit\u001b[1;34m(self, start_params, transformed, includes_fixed, method, method_kwargs, gls, gls_kwargs, cov_type, cov_kwds, return_params, low_memory)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m     method_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 395\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcov_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcov_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov_kwds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcov_kwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmethod_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_params:\n\u001b[0;32m    399\u001b[0m         res\u001b[38;5;241m.\u001b[39mfit_details \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mmlefit\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py:650\u001b[0m, in \u001b[0;36mMLEModel.fit\u001b[1;34m(self, start_params, transformed, includes_fixed, cov_type, cov_kwds, method, maxiter, full_output, disp, callback, return_params, optim_score, optim_complex_step, optim_hessian, flags, low_memory, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;124;03mFits the model by maximum likelihood via Kalman filter.\u001b[39;00m\n\u001b[0;32m    532\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;124;03mstatsmodels.tsa.statespace.structural.UnobservedComponentsResults\u001b[39;00m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m     start_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_params\u001b[49m\n\u001b[0;32m    651\u001b[0m     transformed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    652\u001b[0m     includes_fixed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:953\u001b[0m, in \u001b[0;36mSARIMAX.start_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    949\u001b[0m     params_exog \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    951\u001b[0m \u001b[38;5;66;03m# Non-seasonal ARMA component and trend\u001b[39;00m\n\u001b[0;32m    952\u001b[0m (params_trend, params_ar, params_ma,\n\u001b[1;32m--> 953\u001b[0m  params_variance) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conditional_sum_squares\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_ar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolynomial_ar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_ma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolynomial_ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_k_trend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrend_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarning_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mARMA and trend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[38;5;66;03m# If we have estimated non-stationary start parameters but enforce\u001b[39;00m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;66;03m# stationarity is on, start with 0 parameters and warn\u001b[39;00m\n\u001b[0;32m    960\u001b[0m invalid_ar \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_ar \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menforce_stationarity \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m is_invertible(np\u001b[38;5;241m.\u001b[39mr_[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39mparams_ar])\n\u001b[0;32m    964\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:859\u001b[0m, in \u001b[0;36mSARIMAX._conditional_sum_squares\u001b[1;34m(endog, k_ar, polynomial_ar, k_ma, polynomial_ma, k_trend, trend_data, warning_description)\u001b[0m\n\u001b[0;32m    856\u001b[0m         X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mc_[X, lagmat(residuals, k_ma)[r\u001b[38;5;241m-\u001b[39mk:, cols]]\n\u001b[0;32m    858\u001b[0m     \u001b[38;5;66;03m# Get the array of [ar_params, ma_params]\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpinv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdot(Y)\n\u001b[0;32m    860\u001b[0m     residuals \u001b[38;5;241m=\u001b[39m Y \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X, params)\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2030\u001b[0m, in \u001b[0;36mpinv\u001b[1;34m(a, rcond, hermitian)\u001b[0m\n\u001b[0;32m   2027\u001b[0m s \u001b[38;5;241m=\u001b[39m divide(\u001b[38;5;241m1\u001b[39m, s, where\u001b[38;5;241m=\u001b[39mlarge, out\u001b[38;5;241m=\u001b[39ms)\n\u001b[0;32m   2028\u001b[0m s[\u001b[38;5;241m~\u001b[39mlarge] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 2030\u001b[0m res \u001b[38;5;241m=\u001b[39m matmul(transpose(vt), \u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(res)\n",
            "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 39.7 MiB for an array with shape (5, 1040682) and data type float64"
          ]
        }
      ],
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "# 初始化模型\n",
        "arima_model = ARIMA(y_train, order=(5,1,0))\n",
        "arima_model_fit = arima_model.fit()\n",
        "y_arima_pred = arima_model_fit.forecast(steps=len(y_val))\n",
        "\n",
        "# 評估模型\n",
        "rmsle = np.sqrt(mean_squared_log_error(y_val, y_arima_pred))\n",
        "print(f'ARIMA Model RMSLE: {rmsle}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "特徵工程 - 測試集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 處理嵌入向量欄位\n",
        "def process_embeddings(df, column_name):\n",
        "    embeddings = df[column_name].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=',')).values\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "# 批量處理嵌入向量，避免記憶體問題\n",
        "batch_size = 1000\n",
        "n_batches = len(test_data) // batch_size + 1\n",
        "\n",
        "family_embeddings = []\n",
        "description_embeddings = []\n",
        "\n",
        "for i in range(n_batches):\n",
        "    batch_data = test_data.iloc[i * batch_size: (i + 1) * batch_size]\n",
        "    family_embeddings.append(process_embeddings(batch_data, 'family_bert_embeddings'))\n",
        "    description_embeddings.append(process_embeddings(batch_data, 'description_bert_embeddings'))\n",
        "\n",
        "family_embeddings = np.vstack(family_embeddings)\n",
        "description_embeddings = np.vstack(description_embeddings)\n",
        "\n",
        "# 使用 PCA 將 Word2Vec 向量降維\n",
        "pca = PCA(n_components=10)\n",
        "family_reduced = pca.fit_transform(family_embeddings)\n",
        "description_reduced = pca.fit_transform(description_embeddings)\n",
        "\n",
        "# 將降維後的向量轉換為 DataFrame\n",
        "family_reduced_df = pd.DataFrame(family_reduced, columns=[f'family_{i}' for i in range(10)])\n",
        "description_reduced_df = pd.DataFrame(description_reduced, columns=[f'description_{i}' for i in range(10)])\n",
        "\n",
        "# 合併嵌入向量欄位到原始資料\n",
        "test_data = pd.concat([test_data, family_reduced_df, description_reduced_df], axis=1)\n",
        "test_data.drop(['family_bert_embeddings', 'description_bert_embeddings'], axis=1, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 選擇特徵和目標變數\n",
        "features = ['onpromotion', 'dcoilwtico', 'transactions', 'transferred', 'year', 'month', 'day', 'day_of_week', 'longitude', 'latitude', 'isHoliday', 'isEvent','store_nbr', 'store_type', 'cluster']\n",
        "features += [f'family_{i}' for i in range(10)] + [f'description_{i}' for i in range(10)]\n",
        "target = 'sales'\n",
        "\n",
        "# 處理缺失值\n",
        "test_data = test_data.fillna(0)\n",
        "\n",
        "# 選取需要的欄位資料\n",
        "test_feature_df = test_data[features]\n",
        "\n",
        "# 將資料集中的 'True' 和 'False' 取代為 1 和 0\n",
        "test_feature_df = test_feature_df.replace('True', 1)\n",
        "test_feature_df = test_feature_df.replace('False', 0)\n",
        "\n",
        "# one-hot encoding\n",
        "test_feature_df = pd.get_dummies(test_feature_df, columns=['store_nbr', 'store_type', 'cluster'])\n",
        "\n",
        "X_pred_test = test_feature_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "模型預測並輸出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple Linear Regression\n",
        "\n",
        "# 特徵和目標變數\n",
        "y_pred_test = linear_model.predict(X_pred_test)\n",
        "y_pred_test = postprocess_predictions(y_pred_test)\n",
        "y_test_data = pd.DataFrame({'id': test_data['id'], 'sales': y_pred_test})\n",
        "\n",
        "# 輸出結果\n",
        "y_test_data.to_csv('result_slr.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_15096\\4180290489.py:7: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  y_pred_ma = feature_df['sales_MA'].iloc[-len(test_data):].fillna(method='backfill').values\n"
          ]
        }
      ],
      "source": [
        "# Moving Average \n",
        "\n",
        "window_size = 3  # 根據需要調整視窗大小\n",
        "feature_df['sales_MA'] = train_data['sales'].rolling(window=window_size).mean()\n",
        "\n",
        "# 預測測試集的銷售量\n",
        "y_pred_ma = feature_df['sales_MA'].iloc[-len(test_data):].fillna(method='backfill').values\n",
        "y_pred_ma = np.maximum(y_pred_ma, 0)\n",
        "y_test_data = pd.DataFrame({'id': test_data['id'], 'sales': y_pred_ma})\n",
        "\n",
        "# 輸出結果\n",
        "y_test_data.to_csv('result_ma.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:917: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Exponential Smoothing\n",
        "\n",
        "exponential_smoothing_model = ExponentialSmoothing(train_data['sales'], trend='add', seasonal=None, seasonal_periods=None)\n",
        "exponential_smoothing_fit = exponential_smoothing_model.fit()\n",
        "\n",
        "# 預測測試集的銷售量\n",
        "y_pred_es = exponential_smoothing_fit.forecast(steps=len(test_data)).values\n",
        "y_pred_es = np.maximum(y_pred_es, 0)\n",
        "y_test_data = pd.DataFrame({'id': test_data['id'], 'sales': y_pred_es})\n",
        "\n",
        "# 輸出結果\n",
        "y_test_data.to_csv('result_es.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ARIMA\n",
        "\n",
        "arima_model = ARIMA(train_data['sales'], order=(5, 1, 0))\n",
        "arima_fit = arima_model.fit()\n",
        "\n",
        "# 預測測試集的銷售量\n",
        "y_pred_arima = arima_fit.forecast(steps=len(test_data)).values\n",
        "y_pred_arima = np.maximum(y_pred_arima, 0)\n",
        "y_test_data = pd.DataFrame({'id': test_data['id'], 'sales': y_pred_arima})\n",
        "\n",
        "y_test_data.to_csv('result_arima.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
